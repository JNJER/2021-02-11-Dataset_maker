{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2021-02-09 Dataset Maker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi! I am  [Jean-Nicolas Jérémie](https://github.com/JNJER) and the goal is to create a dataset of images containening a folder with animated and another with non animated. Based on the prediction of deep convolutional neuronal networks like `VGG16`, `ResNet101`, `AlexNet`, `MobileNet` on the [Imagenet](http://image-net.org/) dataset wich allows to work on naturals images for $K = 1000$ labels or even a Single Shot Detection network (`SSD` based on a VGG architecture implemented from a very neat [tutorial](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection.git)) working on the [The PASCAL VOC project](http://host.robots.ox.ac.uk/pascal/VOC/) dataset wich allows to work on naturals images for $K = 20$ labels. Here we decide to try the performances on dataset like [PerrinetBednar15](https://github.com/laurentperrinet/PerrinetBednar15) or [Object and Semantic Images and Eye-tracking (OSIE) data set](https://github.com/NUS-VIP/predicting-human-gaze-beyond-pixels.git). \n",
    "\n",
    "In this notebook, I will use the [Pytorch](https://pytorch.org/) library for running the networks and the [pandas](https://pandas.pydata.org/docs/getting_started/index.html) library to collect and display the results. This notebook was done during a master 2 internship at the Neurosciences Institute of Timone (INT) under the supervision of [Laurent PERRINET](https://laurentperrinet.github.io/). It is curated in the following [github repo](https://github.com/JNJER/2021-01-11_fast_and_curious.git)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%mkdir -p DCNN_dataset_maker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries; definition of the dataset\n",
    "Our coding strategy is to build up a small libray as a package of scripts in the `DCNN_benchmark` folder and to run all calls to that library from this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting DCNN_dataset_maker/init.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile DCNN_dataset_maker/init.py\n",
    "\n",
    "# Importing libraries\n",
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from time import strftime, gmtime\n",
    "import time \n",
    "import json\n",
    "import time \n",
    "import numpy as np\n",
    "import imageio\n",
    "from utils import *\n",
    "from numpy import random\n",
    "import argparse\n",
    "import cv2 \n",
    "import os.path as osp\n",
    "import shutil\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# to plot &  display\n",
    "import matplotlib.pyplot as plt\n",
    "def pprint(message):\n",
    "    print('-'*len(message))\n",
    "    print(message)\n",
    "    print('-'*len(message))\n",
    "\n",
    "# parse the root to the init module\n",
    "def arg_parse(): \n",
    "    parser = argparse.ArgumentParser(description='DCNN_training_benchmark/init.py set root')\n",
    "    parser.add_argument(\"--path_in\", dest = 'path_in', help = \n",
    "                        \"Set the Directory containing images to perform detection upon\",\n",
    "                        default = '/Users/jjn/Desktop/test', type = str)\n",
    "    parser.add_argument(\"--path_out\", dest = 'path_out', help = \n",
    "                        \"Set the Directory containing images to perform detection upon\",\n",
    "                        default = '/Users/jjn/Desktop/test_1', type = str)\n",
    "    parser.add_argument(\"--HOST\", dest = 'HOST', help = \n",
    "                    \"Set the name of your machine\",\n",
    "                    default = os.uname()[1], type = str)\n",
    "    parser.add_argument(\"--datetag\", dest = 'datetag', help = \n",
    "                    \"Set the datetag of the result's file\",\n",
    "                    default = strftime(\"%Y-%m-%d\", gmtime()), type = str)\n",
    "    parser.add_argument(\"--image_size\", dest = 'image_size', help = \n",
    "                    \"Set the image_size of the input\",\n",
    "                    default = 256)\n",
    "    parser.add_argument(\"--image_size_SSD\", dest = 'image_size_SSD', help = \n",
    "                    \"Set the image_size of the input\",\n",
    "                    default = 300)\n",
    "    parser.add_argument(\"--imagenet_label_root\", dest = 'imagenet_label_root', help = \n",
    "                        \"Set the Directory containing imagenet labels\",\n",
    "                        default = 'imagenet_classes.txt', type = str)\n",
    "    parser.add_argument(\"--min_score\", dest = \"min_score\", help = \n",
    "                        \"minimum threshold for a detected box to be considered a match for a certain class\",\n",
    "                        default = 0.6)\n",
    "    parser.add_argument(\"--max_overlap\", dest = \"max_overlap\", help = \n",
    "                        \"maximum overlap two boxes can have so that the one with the lower score is not suppressed via Non-Maximum Suppression (NMS)\",\n",
    "                        default = 0.3)\n",
    "    parser.add_argument(\"--checkpoint\", dest = 'checkpoint', help = \n",
    "                        \"weightsfile\",\n",
    "                        default = \"checkpoint_ssd300.pth.tar\", type = str)\n",
    "    parser.add_argument(\"--top_k\", dest = 'top_k', help = \n",
    "                        \"if there are a lot of resulting detection across all classes, keep only the top 'k'\",\n",
    "                        default = 200 , type = str)\n",
    "    parser.add_argument(\"--class_crop\", dest = 'class_crop', help = \n",
    "                        \"Select a class to trigger the crop\",\n",
    "                        default = \"person\", type = str)\n",
    "    parser.add_argument(\"--class_roll\", dest = 'class_roll', help = \n",
    "                        \"Select a class to trigger the roll\",\n",
    "                        default = \"person\", type = str)\n",
    "    return parser.parse_args()\n",
    "\n",
    "args = arg_parse()\n",
    "\n",
    "# SSD variables \n",
    "min_score = float(args.min_score)\n",
    "max_overlap = float(args.max_overlap)\n",
    "checkpoint = args.checkpoint\n",
    "top_k = float(args.top_k)\n",
    "animated= ( 'bird', 'cat', 'cow', 'dog', 'horse', 'person', 'sheep')\n",
    "class_crop = args.class_crop\n",
    "class_roll = args.class_roll\n",
    "\n",
    "# figure's variables\n",
    "fig_width = 20\n",
    "phi = (np.sqrt(5)+1)/2 # golden ratio\n",
    "phi = phi**2\n",
    "colors = ['b', 'r', 'k','g']\n",
    "\n",
    "# host & date's variables \n",
    "HOST = args.HOST\n",
    "datetag = args.datetag\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#dataset configuration\n",
    "image_size = args.image_size # default image resolution\n",
    "image_size_SSD = args.image_size_SSD # default image resolution\n",
    "id_dl = ''\n",
    "\n",
    "path_in = args.path_in \n",
    "path_out = args.path_out \n",
    "path_out_a = os.path.join(path_out+'/animated')\n",
    "path_out_b = os.path.join(path_out+'/non_animated')\n",
    "path_out_crop = os.path.join(path_out+'/crop')\n",
    "path_out_roll = os.path.join(path_out+'/roll')\n",
    "\n",
    "imagenet_label_root = args.imagenet_label_root\n",
    "with open(imagenet_label_root) as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "labels[0].split(', ')\n",
    "labels = [label.split(', ')[1].lower().replace('_', ' ') for label in labels]\n",
    "\n",
    "def is_animated(x):\n",
    "    if x <=397:\n",
    "        return 1\n",
    "    else:\n",
    "        return\n",
    "pprint('init.py : Done !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "init.py : Done !\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "%run DCNN_dataset_maker/init.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HOST == 'fortytwo':\n",
    "    do_local = False \n",
    "    python_exec = \"KMP_DUPLICATE_LIB_OK=TRUE python3\"\n",
    "else :\n",
    "    do_local =True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating/checking folders to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptname = 'DCNN_dataset_maker/dataset.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting DCNN_dataset_maker/dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "from init import *\n",
    "\n",
    "# check if the folder exist\n",
    "if os.path.isdir(path_out):\n",
    "    list_dir = os.listdir(path_out)\n",
    "    if not \"animated\" in list_dir:\n",
    "        pprint(f\"No existing path match for this folder, creating a folder at {path_out_a}\")\n",
    "        os.makedirs(path_out_a)\n",
    "    if not \"non_animated\" in list_dir:\n",
    "        pprint(f\"No existing path match for this folder, creating a folder at {path_out_b}\")\n",
    "        os.makedirs(path_out_b)\n",
    "    if not \"crop\" in list_dir:\n",
    "        pprint(f\"No existing path match for this folder, creating a folder at {path_out_crop}\")\n",
    "        os.makedirs(path_out_crop)\n",
    "    if not \"roll\" in list_dir:\n",
    "        pprint(f\"No existing path match for this folder, creating a folder at {path_out_roll}\")\n",
    "        os.makedirs(path_out_roll)\n",
    "    else:\n",
    "        pprint(f\"The folder already exists, it includes: {list_dir}\")\n",
    "\n",
    "# no folder, creating one \n",
    "else :\n",
    "    print(f\"No existing path match for this folder, creating a folder at {path_out}\")\n",
    "    os.makedirs(path_out)\n",
    "    os.makedirs(path_out_a)\n",
    "    os.makedirs(path_out_b)\n",
    "    list_dir = os.listdir(path_out)\n",
    "    pprint(f\"Now the folder contains : {os.listdir(path_out)}\")\n",
    "\n",
    "if os.path.isdir(path_in):\n",
    "    list_dir = os.listdir(path_in)\n",
    "    pprint(f\"The folder already exists, it includes: {list_dir}\")\n",
    "else :\n",
    "    pprint(f\"No existing path match for this folder at {path_in} check your data !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "init.py : Done !\n",
      "----------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "No existing path match for this folder, creating a folder at /Users/jjn/Desktop/test_1/non_animated\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "No existing path match for this folder, creating a folder at /Users/jjn/Desktop/test_1/non_animated\n",
      "---------------------------------------------------------------------------------------------------\n",
      "-----------------------------------------------\n",
      "The folder already exists, it includes: ['det']\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if do_local:\n",
    "    %run {scriptname}\n",
    "else:\n",
    "    !python3 {scriptname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained network's import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we worked on five differents pre-trained networks `Alexnet`, `Mobilenet`, `Resnet101`, `VGG16` & `SSD` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptname = 'DCNN_dataset_maker/models.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting DCNN_dataset_maker/models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "from DCNN_dataset_maker.init import *\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "# transform function for input's image processing\n",
    "transform_SSD = transforms.Compose([\n",
    "    transforms.Resize((int(image_size_SSD),int(image_size_SSD))),      # Resize the image to image_size x image_size pixels size.\n",
    "    transforms.ToTensor(),       # Convert the image to PyTorch Tensor data type.\n",
    "    transforms.Normalize(        # Normalize the image by adjusting\n",
    "    mean=[0.485, 0.456, 0.406],  #  its average and\n",
    "    std=[0.229, 0.224, 0.225]    #  its standard deviation at the specified values.              \n",
    "    )])\n",
    "\n",
    "\n",
    "# transform function for input's image processing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((int(image_size))),      # Resize the image to image_size x image_size pixels size.\n",
    "    transforms.ToTensor(),       # Convert the image to PyTorch Tensor data type.\n",
    "    transforms.Normalize(        # Normalize the image by adjusting\n",
    "    mean=[0.485, 0.456, 0.406],  #  its average and\n",
    "    std=[0.229, 0.224, 0.225]    #  its standard deviation at the specified values.              \n",
    "    )])\n",
    "\n",
    "image_dataset = ImageFolder(path_in, transform=transform) # save the dataset\n",
    "image_dataset_SSD = ImageFolder(path_in, transform=transform_SSD) # save the dataset\n",
    "\n",
    "# imports networks with weights\n",
    "models = {} # get model's names\n",
    "\n",
    "# Load SSD model from the checkpoint\n",
    "checkpoint = 'checkpoint_ssd300.pth.tar'\n",
    "try:\n",
    "    checkpoint = torch.load(checkpoint)\n",
    "except:\n",
    "    checkpoint = torch.load(checkpoint, map_location=torch.device('cpu'))\n",
    "pprint('Loaded SSD model')\n",
    "model = checkpoint['model'].to(device).eval()\n",
    "\n",
    "\n",
    "#models['vgg16'] = torchvision.models.vgg16(pretrained=True)\n",
    "print(\"Loading pretrained torchvision's model..\")\n",
    "models['alex'] = torchvision.models.alexnet(pretrained=True)\n",
    "models['vgg'] = torchvision.models.vgg16(pretrained=True)\n",
    "models['mob'] = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "models['res'] = torchvision.models.resnext101_32x8d(pretrained=True)\n",
    "pprint(\"Loaded!\")\n",
    "\n",
    "for name in models.keys():\n",
    "    models[name].to(device).eval()\n",
    "\n",
    "print(datetag, 'Running benchmark on host', HOST, 'with',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "init.py : Done !\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'model.SSD300' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.MaxPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Loaded SSD model\n",
      "----------------\n",
      "Loading pretrained torchvision's model..\n",
      "-------\n",
      "Loaded!\n",
      "-------\n",
      "2021-02-11 Running benchmark on host MacBook-Air-de-jeremie.local with cpu\n"
     ]
    }
   ],
   "source": [
    "%run {scriptname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dataset based on a criterion on the Imagenet Dataset (here animated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptname = 'DCNN_classif.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting DCNN_classif.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "\n",
    "#import model's script and set the output file\n",
    "from DCNN_dataset_maker.models import * \n",
    "i = 0 \n",
    "alive = []\n",
    "for i_image, (data, label) in enumerate(image_dataset):\n",
    "    for name in models.keys():\n",
    "        model = models[name]\n",
    "        tic = time.time()\n",
    "        out = model(data.unsqueeze(0).to(device)).squeeze(0)\n",
    "        percentage = torch.nn.functional.softmax(out, dim=0) * 100\n",
    "        _, indices = torch.sort(out, descending=True)\n",
    "        dt = time.time() - tic  \n",
    "        for idx in indices[:1]:\n",
    "            if percentage[idx].item() > 25 and  idx <=397:\n",
    "                alive.append(1)\n",
    "                #print(image_dataset.imgs[i_image][0], alive)\n",
    "    if len(alive)>=2 : \n",
    "        i+=1\n",
    "        print(f'The {name} model get {labels[idx]} at {percentage[idx].item():.2f} % confidence in {dt:.3f} seconds') # Affichage du meilleur pourcentage\n",
    "        shutil.copy(image_dataset.imgs[i_image][0], path_out_a)\n",
    "    else:\n",
    "        shutil.copy(image_dataset.imgs[i_image][0], path_out_b)   \n",
    "    alive = []\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Loaded SSD model\n",
      "----------------\n",
      "Loading pretrained torchvision's model..\n",
      "-------\n",
      "Loaded!\n",
      "-------\n",
      "2021-02-11 Running benchmark on host MacBook-Air-de-jeremie.local with cpu\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "if do_local:\n",
    "    %run {scriptname} \n",
    "else:\n",
    "    !{python_exec} {scriptname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dataset based on a criterion of the PASCAL VOC project dataset (here animated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptname = 'detect_data_classif.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting detect_data_classif.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "from DCNN_dataset_maker.models import *\n",
    "\n",
    "i = 0 \n",
    "suppress=None\n",
    "\n",
    "for i_image, (data, label) in enumerate(image_dataset_SSD):\n",
    "    \n",
    "        # Forward prop.\n",
    "        predicted_locs, predicted_scores = model(data.unsqueeze(0).to(device)) # Move to default device\n",
    "\n",
    "        # Detect objects in SSD output\n",
    "        det_boxes, det_labels, det_scores = model.detect_objects(predicted_locs, predicted_scores, min_score=min_score,\n",
    "                                                                 max_overlap=max_overlap, top_k=top_k)\n",
    "\n",
    "        # Decode class integer labels\n",
    "        det_labels = [rev_label_map[l] for l in det_labels[0].to('cpu').tolist()]\n",
    "\n",
    "        # Sort by the labels\n",
    "        is_anime = False\n",
    "        for lab_ in det_labels :\n",
    "            if lab_ in animated:\n",
    "                is_anime = True\n",
    "                \n",
    "        if is_anime : \n",
    "            i+=1\n",
    "            print(image_dataset.imgs[i_image][0], is_anime)\n",
    "            shutil.copy(image_dataset.imgs[i_image][0], path_out_a)\n",
    "        else:\n",
    "            print(image_dataset.imgs[i_image][0], is_anime)\n",
    "            shutil.copy(image_dataset.imgs[i_image][0], path_out_b)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jjn/Nextcloud/2021_StageM2_Jean-Nicolas/dev/2021-01-19_YOLO_notebook/2021-01-20_a-PyTorch-Tutorial-to-Object-Detection/model.py:496: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  condition = torch.tensor(condition, dtype=torch.uint8).to(device)\n",
      "/Users/jjn/Nextcloud/2021_StageM2_Jean-Nicolas/dev/2021-01-19_YOLO_notebook/2021-01-20_a-PyTorch-Tutorial-to-Object-Detection/model.py:504: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:25.)\n",
      "  image_boxes.append(class_decoded_locs[1 - suppress])\n",
      "/Users/jjn/Nextcloud/2021_StageM2_Jean-Nicolas/dev/2021-01-19_YOLO_notebook/2021-01-20_a-PyTorch-Tutorial-to-Object-Detection/model.py:506: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:25.)\n",
      "  image_scores.append(class_scores[1 - suppress])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jjn/Desktop/test/det/1332.jpg True\n",
      "/Users/jjn/Desktop/test/det/1357.jpg False\n",
      "/Users/jjn/Desktop/test/det/1388.jpg True\n",
      "/Users/jjn/Desktop/test/det/1403.jpg False\n",
      "/Users/jjn/Desktop/test/det/1412.jpg False\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "if do_local:\n",
    "    %run {scriptname}\n",
    "else:\n",
    "    !{python_exec} {scriptname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dataset croped on a class based on the PASCAL VOC project dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptname = 'detect_crop.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting detect_crop.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "from DCNN_dataset_maker.models import *\n",
    "\n",
    "for i_image, (data, label) in enumerate(image_dataset_SSD):\n",
    "    # Forward prop.\n",
    "    predicted_locs, predicted_scores = model(data.unsqueeze(0).to(device)) # Move to default device\n",
    "\n",
    "    # Detect objects in SSD output\n",
    "    det_boxes, det_labels, det_scores = model.detect_objects(predicted_locs, predicted_scores, min_score=min_score,\n",
    "                                                             max_overlap=max_overlap, top_k=top_k)\n",
    "    # Move detections to the CPU\n",
    "    det_boxes = det_boxes[0].to('cpu')\n",
    "\n",
    "    # Transform to original image dimensions\n",
    "    original_image = Image.open(image_dataset.imgs[i_image][0], mode='r')\n",
    "    original_dims = torch.FloatTensor(\n",
    "        [original_image.width, original_image.height, original_image.width, original_image.height]).unsqueeze(0)\n",
    "    det_boxes = det_boxes * original_dims\n",
    "\n",
    "    # Decode class integer labels\n",
    "    det_labels = [rev_label_map[l] for l in det_labels[0].to('cpu').tolist()]\n",
    "\n",
    "    # If no objects found, the detected labels will be set to ['0.'], i.e. ['background'] in SSD300.detect_objects() in model.py\n",
    "    annotated_images = []\n",
    "    i=0\n",
    "    for x in det_labels:\n",
    "        if x == class_crop :\n",
    "            left =  float(det_boxes[i][0])\n",
    "            upper = float(det_boxes[i][1])\n",
    "            right = float(det_boxes[i][2])\n",
    "            lower = float(det_boxes[i][3])\n",
    "            annotated_image = original_image.crop((left, upper, right, lower))\n",
    "            i += 1\n",
    "            print(f'There is {i} {class_crop}(s) in {image_dataset.imgs[i_image][0]}.')\n",
    "            annotated_image.save(f'{path_out_crop}/{i_image}_{i}.jpg')\n",
    "        else:\n",
    "             print (f\"No {class_crop} in {image_dataset.imgs[i_image][0]}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 1 person(s) in /Users/jjn/Desktop/test/det/1332.jpg.\n",
      "No person in /Users/jjn/Desktop/test/det/1332.jpg\n",
      "No person in /Users/jjn/Desktop/test/det/1357.jpg\n",
      "There is 1 person(s) in /Users/jjn/Desktop/test/det/1388.jpg.\n",
      "No person in /Users/jjn/Desktop/test/det/1388.jpg\n",
      "No person in /Users/jjn/Desktop/test/det/1403.jpg\n",
      "No person in /Users/jjn/Desktop/test/det/1403.jpg\n",
      "No person in /Users/jjn/Desktop/test/det/1403.jpg\n",
      "No person in /Users/jjn/Desktop/test/det/1412.jpg\n"
     ]
    }
   ],
   "source": [
    "if do_local:\n",
    "    %run {scriptname}\n",
    "else:\n",
    "    !{python_exec} {scriptname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset centered on a class based on the PASCAL VOC project dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptname = 'detect_roll.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting detect_roll.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "from DCNN_dataset_maker.models import *\n",
    "\n",
    "\n",
    "for i_image, (data, label) in enumerate(image_dataset_SSD):\n",
    "    \n",
    "    # Forward prop.\n",
    "    predicted_locs, predicted_scores = model(data.unsqueeze(0).to(device)) # Move to default device\n",
    "\n",
    "\n",
    "    # Detect objects in SSD output\n",
    "    det_boxes, det_labels, det_scores = model.detect_objects(predicted_locs, predicted_scores, min_score=min_score,\n",
    "                                                             max_overlap=max_overlap, top_k=top_k)\n",
    "    \n",
    "    # Move detections to the CPU\n",
    "    det_boxes = det_boxes[0].to('cpu')\n",
    "\n",
    "    # Transform to original image dimensions\n",
    "    original_image = Image.open(image_dataset.imgs[i_image][0], mode='r')\n",
    "    N_X = original_image.width # Get the size of the picture\n",
    "    N_Y = original_image.height\n",
    "    original_dims = torch.FloatTensor(\n",
    "        [N_X, N_Y, N_X, N_Y]).unsqueeze(0)\n",
    "    det_boxes = det_boxes * original_dims\n",
    "\n",
    "    # Decode class integer labels\n",
    "    det_labels = [rev_label_map[l] for l in det_labels[0].to('cpu').tolist()]\n",
    "\n",
    "    # Choose the region of interest as the highest saliency for a given label\n",
    "    det_score = det_scores[0].tolist()\n",
    "    ROI = det_score.index(max(det_score)) \n",
    "    if class_roll in det_labels:\n",
    "        if det_labels[ROI] != class_roll:\n",
    "            det_score[ROI] = 0\n",
    "            ROI = det_score.index(max(det_score))    \n",
    "\n",
    "    for x in det_labels:\n",
    "        if x == class_roll : \n",
    "            left =  float(det_boxes[ROI][0]) #Get the position of the bounding box\n",
    "            upper = float(det_boxes[ROI][1])\n",
    "            right = float(det_boxes[ROI][2])\n",
    "            lower = float(det_boxes[ROI][3])\n",
    "            c1 = int((N_Y//2)-(((lower-upper)//2)+upper)) # \"distance\" from the center of the picture to the center of the box on the 0 axis\n",
    "            c2 = int((N_X//2)-(((right-left)//2)+left)) # \"distance\" from the center of the picture to the center of the box on the 1 axis\n",
    "            annotated_image = np.roll(original_image, c2, axis=1) # sliding the gaze to the right by moving the picture to the left\n",
    "            annotated_image = np.roll(annotated_image, c1, axis=0)# sliding the gaze up by moving the picture to the bottom\n",
    "            print(f'There is a {class_roll}(s) in {image_dataset.imgs[i_image][0]}.')\n",
    "            annotated_image = Image.fromarray(np.uint8(annotated_image)).save(f'{path_out_roll}/{i_image}.jpg')\n",
    "        else:\n",
    "            print (f\"No {class_roll} in {image_dataset.imgs[i_image][0]}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a person(s) in /Users/jjn/Desktop/test/det/1332.jpg.\n",
      "No person in /Users/jjn/Desktop/test/det/1332.jpg\n",
      "No person in /Users/jjn/Desktop/test/det/1357.jpg\n",
      "There is a person(s) in /Users/jjn/Desktop/test/det/1388.jpg.\n",
      "No person in /Users/jjn/Desktop/test/det/1388.jpg\n",
      "No person in /Users/jjn/Desktop/test/det/1403.jpg\n",
      "No person in /Users/jjn/Desktop/test/det/1403.jpg\n",
      "No person in /Users/jjn/Desktop/test/det/1403.jpg\n",
      "No person in /Users/jjn/Desktop/test/det/1412.jpg\n"
     ]
    }
   ],
   "source": [
    "if do_local:\n",
    "    %run {scriptname}\n",
    "else:\n",
    "    !{python_exec} {scriptname}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
